{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bisection\n",
    "\n",
    "\n",
    "One of the most common algorithms for numerical root-finding is *bisection*.\n",
    "\n",
    "To understand the idea, recall the well-known game where:\n",
    "\n",
    "- Player A thinks of a secret number between 1 and 100  \n",
    "- Player B asks if it’s less than 50  \n",
    "  \n",
    "  - If yes, B asks if it’s less than 25  \n",
    "  - If no, B asks if it’s less than 75  \n",
    "  \n",
    "\n",
    "And so on.\n",
    "\n",
    "This is bisection, a relative of [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm). It works for all sufficiently well behaved increasing continuous functions with $ f(a) < 0 < f(b) $. \n",
    "\n",
    "Write an implementation of the bisection algorith, `bisect(f, lower, upper, tol)` which, given a function `f`, a lower bound `lower` and an upper bound `upper` finds the point `x` where `f(x) = 0`. The parameter `tol` is a numerical tolerance, you should stop once your step size is smaller than `tol`.\n",
    "\n",
    "\n",
    "Use it to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1 \\tag{2}\n",
    "$$\n",
    "\n",
    "in python: `lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1`\n",
    "\n",
    "The value where f(x) = 0 should be around `0.408`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408203125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "\n",
    "def bisect(f, lower, upper, tol):\n",
    "    assert f(lower) * f(upper) < 0\n",
    "    \n",
    "    while (upper - lower) / 2 > tol:\n",
    "        midpoint = (lower + upper)/2\n",
    "        if f(midpoint) == 0:\n",
    "            return(midpoint)\n",
    "        elif f(upper) * f(midpoint) < 0:\n",
    "            lower = midpoint\n",
    "        else:\n",
    "            upper = midpoint\n",
    "\n",
    "    return midpoint\n",
    "\n",
    "bisect(f,-1,1,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 (stretch) Recursive Bisect\n",
    "\n",
    "Write a recursive version of the bisection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Movies Regression\n",
    "\n",
    "Write the best linear regression model you can on the [Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset?select=ratings.csv) to predict the profitability of a movie (revenue - budget). Maintain the interpretability of the model.\n",
    "\n",
    "Few notes:\n",
    "\n",
    "1. Clean your data! Movies where the budget or revenue are invalid should be thrown out\n",
    "\n",
    "2. Be creative with feature engineering. You can include processing to one-hot encode the type of movie, etc.\n",
    "\n",
    "3. The model should be useful for someone **who is thinking about making a movie**. So features like the popularity can't be used. You could, however, use the ratings to figure out if making \"good\" or \"oscar bait\" movies is a profitable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPi9xcx3mywk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv('archive/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANUP\n",
    "#Drop unnecessary columns \n",
    "\n",
    "df = df.drop(columns=['belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview', 'poster_path',\n",
    "                     'spoken_languages', 'tagline', 'video', 'production_countries', 'popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove movies that are unreleased or cancelled\n",
    "\n",
    "df = df[df['status'] == 'Released']\n",
    "\n",
    "#Clean genres column and take the first genre of each movie for simplicity\n",
    "\n",
    "df.genres = df.genres.apply(lambda x: eval(x))\n",
    "df.genres = df.genres.apply(lambda x: x[0]['name'] if len(x) > 0 else '')\n",
    "\n",
    "#Remove blank genre columns\n",
    "df = df[(df.genres != '')]\n",
    "\n",
    "#Create return column\n",
    "df.budget = df.budget.astype('float')\n",
    "df = df[(df.budget != 0) & (df.revenue != 0)]\n",
    "df.adult = df.adult.replace('False', 0).replace('True', 1)\n",
    "df['return'] = df.revenue - df.budget\n",
    "df = df.reset_index().drop(columns=['index'])\n",
    "\n",
    "#Get dummies for genre and language\n",
    "genres = pd.get_dummies(df.genres)\n",
    "lang = pd.get_dummies(df.original_language)\n",
    "\n",
    "#New DF for the regression variables that are to be concatenated with the dummies\n",
    "Vars = ['adult', 'revenue', 'runtime', 'vote_average', 'return']\n",
    "Var = df[Vars]\n",
    "\n",
    "#Concatenate everything\n",
    "df = pd.concat([Var, genres, lang], axis=1)\n",
    "\n",
    "#Get rid of duplicates\n",
    "df = df.T.groupby(level=0).first().T\n",
    "\n",
    "#One NAN found so just drop it\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>return</td>      <th>  R-squared:         </th> <td>   0.967</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.967</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2572.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 05 Feb 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:44:55</td>     <th>  Log-Likelihood:    </th> <td> -99039.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5364</td>      <th>  AIC:               </th> <td>1.982e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5302</td>      <th>  BIC:               </th> <td>1.986e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    61</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-1.797e+07</td> <td> 3.92e+06</td> <td>   -4.584</td> <td> 0.000</td> <td>-2.57e+07</td> <td>-1.03e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Action</th>          <td> -9.51e+06</td> <td> 1.68e+06</td> <td>   -5.666</td> <td> 0.000</td> <td>-1.28e+07</td> <td>-6.22e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Adventure</th>       <td>-1.577e+07</td> <td> 1.92e+06</td> <td>   -8.214</td> <td> 0.000</td> <td>-1.95e+07</td> <td> -1.2e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Animation</th>       <td>-1.977e+07</td> <td> 2.55e+06</td> <td>   -7.739</td> <td> 0.000</td> <td>-2.48e+07</td> <td>-1.48e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Comedy</th>          <td> 3.144e+06</td> <td> 1.66e+06</td> <td>    1.895</td> <td> 0.058</td> <td>-1.08e+05</td> <td>  6.4e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Crime</th>           <td>-2.622e+05</td> <td> 2.11e+06</td> <td>   -0.124</td> <td> 0.901</td> <td>-4.41e+06</td> <td> 3.88e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Documentary</th>     <td> 8.501e+06</td> <td> 3.85e+06</td> <td>    2.206</td> <td> 0.027</td> <td> 9.45e+05</td> <td> 1.61e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Drama</th>           <td> 3.008e+06</td> <td> 1.66e+06</td> <td>    1.816</td> <td> 0.069</td> <td>-2.39e+05</td> <td> 6.26e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Family</th>          <td>-9.124e+06</td> <td> 3.62e+06</td> <td>   -2.522</td> <td> 0.012</td> <td>-1.62e+07</td> <td>-2.03e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fantasy</th>         <td>-1.232e+07</td> <td> 2.54e+06</td> <td>   -4.855</td> <td> 0.000</td> <td>-1.73e+07</td> <td>-7.35e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Foreign</th>         <td> 1.945e+07</td> <td> 1.22e+07</td> <td>    1.589</td> <td> 0.112</td> <td>-4.55e+06</td> <td> 4.34e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>History</th>         <td> 3.774e+06</td> <td> 4.77e+06</td> <td>    0.792</td> <td> 0.428</td> <td>-5.57e+06</td> <td> 1.31e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Horror</th>          <td> 9.644e+06</td> <td>    2e+06</td> <td>    4.811</td> <td> 0.000</td> <td> 5.71e+06</td> <td> 1.36e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Music</th>           <td> 1.363e+06</td> <td> 4.47e+06</td> <td>    0.305</td> <td> 0.760</td> <td>-7.39e+06</td> <td> 1.01e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mystery</th>         <td> 7.192e+05</td> <td> 3.31e+06</td> <td>    0.217</td> <td> 0.828</td> <td>-5.77e+06</td> <td> 7.21e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Romance</th>         <td> 3.294e+06</td> <td> 2.66e+06</td> <td>    1.238</td> <td> 0.216</td> <td>-1.92e+06</td> <td> 8.51e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Science Fiction</th> <td>  -7.1e+06</td> <td>  2.8e+06</td> <td>   -2.532</td> <td> 0.011</td> <td>-1.26e+07</td> <td> -1.6e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV Movie</th>        <td> 1.443e+07</td> <td> 2.42e+07</td> <td>    0.596</td> <td> 0.551</td> <td> -3.3e+07</td> <td> 6.19e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Thriller</th>        <td>-9.496e+05</td> <td> 2.27e+06</td> <td>   -0.418</td> <td> 0.676</td> <td> -5.4e+06</td> <td>  3.5e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>War</th>             <td> -1.01e+07</td> <td> 4.27e+06</td> <td>   -2.365</td> <td> 0.018</td> <td>-1.85e+07</td> <td>-1.73e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Western</th>         <td>-3.952e+05</td> <td> 4.65e+06</td> <td>   -0.085</td> <td> 0.932</td> <td>-9.51e+06</td> <td> 8.72e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>adult</th>           <td>    0.6059</td> <td>    8.435</td> <td>    0.072</td> <td> 0.943</td> <td>  -15.930</td> <td>   17.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>af</th>              <td>-6.349e+06</td> <td> 2.49e+07</td> <td>   -0.255</td> <td> 0.799</td> <td>-5.53e+07</td> <td> 4.26e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bm</th>              <td>-1.186e+06</td> <td> 2.49e+07</td> <td>   -0.048</td> <td> 0.962</td> <td>   -5e+07</td> <td> 4.76e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ca</th>              <td>-9.912e+06</td> <td> 2.49e+07</td> <td>   -0.398</td> <td> 0.691</td> <td>-5.87e+07</td> <td> 3.89e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cn</th>              <td>-4.417e+06</td> <td> 6.86e+06</td> <td>   -0.643</td> <td> 0.520</td> <td>-1.79e+07</td> <td> 9.04e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>da</th>              <td>-5.206e+06</td> <td> 7.87e+06</td> <td>   -0.662</td> <td> 0.508</td> <td>-2.06e+07</td> <td> 1.02e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>de</th>              <td>-6.352e+06</td> <td> 5.82e+06</td> <td>   -1.091</td> <td> 0.275</td> <td>-1.78e+07</td> <td> 5.06e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>el</th>              <td>-1.615e+06</td> <td> 2.49e+07</td> <td>   -0.065</td> <td> 0.948</td> <td>-5.04e+07</td> <td> 4.72e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>en</th>              <td>-1.301e+07</td> <td> 2.47e+06</td> <td>   -5.260</td> <td> 0.000</td> <td>-1.79e+07</td> <td>-8.16e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>es</th>              <td>-2.898e+06</td> <td> 4.71e+06</td> <td>   -0.616</td> <td> 0.538</td> <td>-1.21e+07</td> <td> 6.33e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fa</th>              <td>-1.578e+07</td> <td> 2.49e+07</td> <td>   -0.633</td> <td> 0.526</td> <td>-6.46e+07</td> <td> 3.31e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fi</th>              <td> 1.831e+06</td> <td> 1.45e+07</td> <td>    0.126</td> <td> 0.900</td> <td>-2.66e+07</td> <td> 3.03e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fr</th>              <td>-7.343e+06</td> <td> 3.58e+06</td> <td>   -2.050</td> <td> 0.040</td> <td>-1.44e+07</td> <td> -3.2e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>he</th>              <td>-1.096e+07</td> <td> 1.26e+07</td> <td>   -0.867</td> <td> 0.386</td> <td>-3.57e+07</td> <td> 1.38e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hi</th>              <td> 1.034e+07</td> <td> 3.57e+06</td> <td>    2.898</td> <td> 0.004</td> <td> 3.34e+06</td> <td> 1.73e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hu</th>              <td> 2.528e+06</td> <td> 1.78e+07</td> <td>    0.142</td> <td> 0.887</td> <td>-3.24e+07</td> <td> 3.74e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>id</th>              <td>-6.801e+04</td> <td> 1.46e+07</td> <td>   -0.005</td> <td> 0.996</td> <td>-2.86e+07</td> <td> 2.85e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is</th>              <td>-1.055e+07</td> <td> 2.49e+07</td> <td>   -0.424</td> <td> 0.672</td> <td>-5.94e+07</td> <td> 3.83e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>it</th>              <td> -7.86e+06</td> <td> 5.19e+06</td> <td>   -1.515</td> <td> 0.130</td> <td> -1.8e+07</td> <td> 2.31e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ja</th>              <td> 4.457e+06</td> <td>  4.7e+06</td> <td>    0.949</td> <td> 0.343</td> <td>-4.75e+06</td> <td> 1.37e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kn</th>              <td> 1.882e+07</td> <td>  2.5e+07</td> <td>    0.753</td> <td> 0.451</td> <td>-3.02e+07</td> <td> 6.78e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ko</th>              <td>-6.553e+05</td> <td> 5.53e+06</td> <td>   -0.119</td> <td> 0.906</td> <td>-1.15e+07</td> <td> 1.02e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ml</th>              <td>  1.11e+07</td> <td>  7.9e+06</td> <td>    1.406</td> <td> 0.160</td> <td>-4.38e+06</td> <td> 2.66e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mr</th>              <td>-4.042e+06</td> <td> 2.53e+07</td> <td>   -0.160</td> <td> 0.873</td> <td>-5.36e+07</td> <td> 4.55e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nb</th>              <td> 8.053e+06</td> <td>  2.5e+07</td> <td>    0.322</td> <td> 0.747</td> <td>-4.09e+07</td> <td>  5.7e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nl</th>              <td>-3.043e+06</td> <td> 9.68e+06</td> <td>   -0.314</td> <td> 0.753</td> <td> -2.2e+07</td> <td> 1.59e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>no</th>              <td> 5.026e+06</td> <td> 1.27e+07</td> <td>    0.397</td> <td> 0.692</td> <td>-1.98e+07</td> <td> 2.99e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pl</th>              <td>-6.396e+06</td> <td> 1.26e+07</td> <td>   -0.506</td> <td> 0.613</td> <td>-3.12e+07</td> <td> 1.84e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pt</th>              <td>-1.369e+07</td> <td> 9.69e+06</td> <td>   -1.414</td> <td> 0.158</td> <td>-3.27e+07</td> <td>  5.3e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>revenue</th>         <td>    0.8411</td> <td>    0.002</td> <td>  359.855</td> <td> 0.000</td> <td>    0.836</td> <td>    0.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ro</th>              <td>-3.911e+06</td> <td> 1.26e+07</td> <td>   -0.310</td> <td> 0.757</td> <td>-2.87e+07</td> <td> 2.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ru</th>              <td> 4.834e+06</td> <td> 3.91e+06</td> <td>    1.235</td> <td> 0.217</td> <td>-2.84e+06</td> <td> 1.25e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>runtime</th>         <td>-2.628e+05</td> <td> 1.91e+04</td> <td>  -13.744</td> <td> 0.000</td> <td>   -3e+05</td> <td>-2.25e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sr</th>              <td> 7.685e+05</td> <td> 1.46e+07</td> <td>    0.053</td> <td> 0.958</td> <td>-2.78e+07</td> <td> 2.93e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sv</th>              <td>-2.185e+06</td> <td>  9.1e+06</td> <td>   -0.240</td> <td> 0.810</td> <td>   -2e+07</td> <td> 1.57e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ta</th>              <td> 1.409e+07</td> <td> 5.53e+06</td> <td>    2.548</td> <td> 0.011</td> <td> 3.25e+06</td> <td> 2.49e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>te</th>              <td> 1.133e+07</td> <td> 9.14e+06</td> <td>    1.240</td> <td> 0.215</td> <td>-6.59e+06</td> <td> 2.93e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>th</th>              <td> 9.258e+06</td> <td> 1.77e+07</td> <td>    0.523</td> <td> 0.601</td> <td>-2.54e+07</td> <td>  4.4e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tr</th>              <td>-4.248e+06</td> <td> 1.26e+07</td> <td>   -0.336</td> <td> 0.737</td> <td> -2.9e+07</td> <td> 2.05e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ur</th>              <td> 5.569e+06</td> <td> 1.78e+07</td> <td>    0.313</td> <td> 0.754</td> <td>-2.93e+07</td> <td> 4.05e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vi</th>              <td> 1.456e+07</td> <td> 2.49e+07</td> <td>    0.584</td> <td> 0.559</td> <td>-3.43e+07</td> <td> 6.34e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_average</th>    <td> 6.936e+06</td> <td> 4.22e+05</td> <td>   16.418</td> <td> 0.000</td> <td> 6.11e+06</td> <td> 7.76e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>xx</th>              <td> 3.646e+06</td> <td> 1.77e+07</td> <td>    0.206</td> <td> 0.837</td> <td>-3.11e+07</td> <td> 3.84e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zh</th>              <td>-2.503e+06</td> <td> 5.21e+06</td> <td>   -0.480</td> <td> 0.631</td> <td>-1.27e+07</td> <td> 7.72e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1372.049</td> <th>  Durbin-Watson:     </th> <td>   1.839</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14747.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.910</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>10.916</td>  <th>  Cond. No.          </th> <td>9.92e+21</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.95e-24. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 return   R-squared:                       0.967\n",
       "Model:                            OLS   Adj. R-squared:                  0.967\n",
       "Method:                 Least Squares   F-statistic:                     2572.\n",
       "Date:                Fri, 05 Feb 2021   Prob (F-statistic):               0.00\n",
       "Time:                        16:44:55   Log-Likelihood:                -99039.\n",
       "No. Observations:                5364   AIC:                         1.982e+05\n",
       "Df Residuals:                    5302   BIC:                         1.986e+05\n",
       "Df Model:                          61                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -1.797e+07   3.92e+06     -4.584      0.000   -2.57e+07   -1.03e+07\n",
       "Action           -9.51e+06   1.68e+06     -5.666      0.000   -1.28e+07   -6.22e+06\n",
       "Adventure       -1.577e+07   1.92e+06     -8.214      0.000   -1.95e+07    -1.2e+07\n",
       "Animation       -1.977e+07   2.55e+06     -7.739      0.000   -2.48e+07   -1.48e+07\n",
       "Comedy           3.144e+06   1.66e+06      1.895      0.058   -1.08e+05     6.4e+06\n",
       "Crime           -2.622e+05   2.11e+06     -0.124      0.901   -4.41e+06    3.88e+06\n",
       "Documentary      8.501e+06   3.85e+06      2.206      0.027    9.45e+05    1.61e+07\n",
       "Drama            3.008e+06   1.66e+06      1.816      0.069   -2.39e+05    6.26e+06\n",
       "Family          -9.124e+06   3.62e+06     -2.522      0.012   -1.62e+07   -2.03e+06\n",
       "Fantasy         -1.232e+07   2.54e+06     -4.855      0.000   -1.73e+07   -7.35e+06\n",
       "Foreign          1.945e+07   1.22e+07      1.589      0.112   -4.55e+06    4.34e+07\n",
       "History          3.774e+06   4.77e+06      0.792      0.428   -5.57e+06    1.31e+07\n",
       "Horror           9.644e+06      2e+06      4.811      0.000    5.71e+06    1.36e+07\n",
       "Music            1.363e+06   4.47e+06      0.305      0.760   -7.39e+06    1.01e+07\n",
       "Mystery          7.192e+05   3.31e+06      0.217      0.828   -5.77e+06    7.21e+06\n",
       "Romance          3.294e+06   2.66e+06      1.238      0.216   -1.92e+06    8.51e+06\n",
       "Science Fiction   -7.1e+06    2.8e+06     -2.532      0.011   -1.26e+07    -1.6e+06\n",
       "TV Movie         1.443e+07   2.42e+07      0.596      0.551    -3.3e+07    6.19e+07\n",
       "Thriller        -9.496e+05   2.27e+06     -0.418      0.676    -5.4e+06     3.5e+06\n",
       "War              -1.01e+07   4.27e+06     -2.365      0.018   -1.85e+07   -1.73e+06\n",
       "Western         -3.952e+05   4.65e+06     -0.085      0.932   -9.51e+06    8.72e+06\n",
       "adult               0.6059      8.435      0.072      0.943     -15.930      17.141\n",
       "af              -6.349e+06   2.49e+07     -0.255      0.799   -5.53e+07    4.26e+07\n",
       "bm              -1.186e+06   2.49e+07     -0.048      0.962      -5e+07    4.76e+07\n",
       "ca              -9.912e+06   2.49e+07     -0.398      0.691   -5.87e+07    3.89e+07\n",
       "cn              -4.417e+06   6.86e+06     -0.643      0.520   -1.79e+07    9.04e+06\n",
       "da              -5.206e+06   7.87e+06     -0.662      0.508   -2.06e+07    1.02e+07\n",
       "de              -6.352e+06   5.82e+06     -1.091      0.275   -1.78e+07    5.06e+06\n",
       "el              -1.615e+06   2.49e+07     -0.065      0.948   -5.04e+07    4.72e+07\n",
       "en              -1.301e+07   2.47e+06     -5.260      0.000   -1.79e+07   -8.16e+06\n",
       "es              -2.898e+06   4.71e+06     -0.616      0.538   -1.21e+07    6.33e+06\n",
       "fa              -1.578e+07   2.49e+07     -0.633      0.526   -6.46e+07    3.31e+07\n",
       "fi               1.831e+06   1.45e+07      0.126      0.900   -2.66e+07    3.03e+07\n",
       "fr              -7.343e+06   3.58e+06     -2.050      0.040   -1.44e+07    -3.2e+05\n",
       "he              -1.096e+07   1.26e+07     -0.867      0.386   -3.57e+07    1.38e+07\n",
       "hi               1.034e+07   3.57e+06      2.898      0.004    3.34e+06    1.73e+07\n",
       "hu               2.528e+06   1.78e+07      0.142      0.887   -3.24e+07    3.74e+07\n",
       "id              -6.801e+04   1.46e+07     -0.005      0.996   -2.86e+07    2.85e+07\n",
       "is              -1.055e+07   2.49e+07     -0.424      0.672   -5.94e+07    3.83e+07\n",
       "it               -7.86e+06   5.19e+06     -1.515      0.130    -1.8e+07    2.31e+06\n",
       "ja               4.457e+06    4.7e+06      0.949      0.343   -4.75e+06    1.37e+07\n",
       "kn               1.882e+07    2.5e+07      0.753      0.451   -3.02e+07    6.78e+07\n",
       "ko              -6.553e+05   5.53e+06     -0.119      0.906   -1.15e+07    1.02e+07\n",
       "ml                1.11e+07    7.9e+06      1.406      0.160   -4.38e+06    2.66e+07\n",
       "mr              -4.042e+06   2.53e+07     -0.160      0.873   -5.36e+07    4.55e+07\n",
       "nb               8.053e+06    2.5e+07      0.322      0.747   -4.09e+07     5.7e+07\n",
       "nl              -3.043e+06   9.68e+06     -0.314      0.753    -2.2e+07    1.59e+07\n",
       "no               5.026e+06   1.27e+07      0.397      0.692   -1.98e+07    2.99e+07\n",
       "pl              -6.396e+06   1.26e+07     -0.506      0.613   -3.12e+07    1.84e+07\n",
       "pt              -1.369e+07   9.69e+06     -1.414      0.158   -3.27e+07     5.3e+06\n",
       "revenue             0.8411      0.002    359.855      0.000       0.836       0.846\n",
       "ro              -3.911e+06   1.26e+07     -0.310      0.757   -2.87e+07    2.09e+07\n",
       "ru               4.834e+06   3.91e+06      1.235      0.217   -2.84e+06    1.25e+07\n",
       "runtime         -2.628e+05   1.91e+04    -13.744      0.000      -3e+05   -2.25e+05\n",
       "sr               7.685e+05   1.46e+07      0.053      0.958   -2.78e+07    2.93e+07\n",
       "sv              -2.185e+06    9.1e+06     -0.240      0.810      -2e+07    1.57e+07\n",
       "ta               1.409e+07   5.53e+06      2.548      0.011    3.25e+06    2.49e+07\n",
       "te               1.133e+07   9.14e+06      1.240      0.215   -6.59e+06    2.93e+07\n",
       "th               9.258e+06   1.77e+07      0.523      0.601   -2.54e+07     4.4e+07\n",
       "tr              -4.248e+06   1.26e+07     -0.336      0.737    -2.9e+07    2.05e+07\n",
       "ur               5.569e+06   1.78e+07      0.313      0.754   -2.93e+07    4.05e+07\n",
       "vi               1.456e+07   2.49e+07      0.584      0.559   -3.43e+07    6.34e+07\n",
       "vote_average     6.936e+06   4.22e+05     16.418      0.000    6.11e+06    7.76e+06\n",
       "xx               3.646e+06   1.77e+07      0.206      0.837   -3.11e+07    3.84e+07\n",
       "zh              -2.503e+06   5.21e+06     -0.480      0.631   -1.27e+07    7.72e+06\n",
       "==============================================================================\n",
       "Omnibus:                     1372.049   Durbin-Watson:                   1.839\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14747.551\n",
       "Skew:                          -0.910   Prob(JB):                         0.00\n",
       "Kurtosis:                      10.916   Cond. No.                     9.92e+21\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.95e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally make the model\n",
    "\n",
    "X = df.drop(columns=['return'])\n",
    "y = df['return']\n",
    "x = sm.add_constant(X)\n",
    "\n",
    "est = sm.OLS(y, x).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>return</td>      <th>  R-squared:         </th> <td>   0.967</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.967</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9834.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 05 Feb 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:44:55</td>     <th>  Log-Likelihood:    </th> <td> -99053.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5364</td>      <th>  AIC:               </th> <td>1.981e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5347</td>      <th>  BIC:               </th> <td>1.983e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    16</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-1.561e+07</td> <td> 3.19e+06</td> <td>   -4.890</td> <td> 0.000</td> <td>-2.19e+07</td> <td>-9.35e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Action</th>          <td>-1.186e+07</td> <td>  9.6e+05</td> <td>  -12.348</td> <td> 0.000</td> <td>-1.37e+07</td> <td>-9.97e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Adventure</th>       <td>-1.799e+07</td> <td> 1.36e+06</td> <td>  -13.218</td> <td> 0.000</td> <td>-2.07e+07</td> <td>-1.53e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Animation</th>       <td>-2.156e+07</td> <td> 2.24e+06</td> <td>   -9.640</td> <td> 0.000</td> <td>-2.59e+07</td> <td>-1.72e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Documentary</th>     <td> 6.133e+06</td> <td> 3.74e+06</td> <td>    1.640</td> <td> 0.101</td> <td> -1.2e+06</td> <td> 1.35e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Family</th>          <td>-1.115e+07</td> <td> 3.49e+06</td> <td>   -3.192</td> <td> 0.001</td> <td> -1.8e+07</td> <td> -4.3e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fantasy</th>         <td>-1.421e+07</td> <td>  2.2e+06</td> <td>   -6.450</td> <td> 0.000</td> <td>-1.85e+07</td> <td>-9.89e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Horror</th>          <td> 7.271e+06</td> <td>  1.5e+06</td> <td>    4.838</td> <td> 0.000</td> <td> 4.32e+06</td> <td> 1.02e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Science Fiction</th> <td>-9.354e+06</td> <td> 2.54e+06</td> <td>   -3.680</td> <td> 0.000</td> <td>-1.43e+07</td> <td>-4.37e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>War</th>             <td>-1.191e+07</td> <td> 4.15e+06</td> <td>   -2.869</td> <td> 0.004</td> <td>   -2e+07</td> <td>-3.77e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>en</th>              <td>-1.259e+07</td> <td>  1.4e+06</td> <td>   -8.997</td> <td> 0.000</td> <td>-1.53e+07</td> <td>-9.85e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fr</th>              <td>-6.644e+06</td> <td>    3e+06</td> <td>   -2.213</td> <td> 0.027</td> <td>-1.25e+07</td> <td>-7.57e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hi</th>              <td> 1.045e+07</td> <td> 2.97e+06</td> <td>    3.523</td> <td> 0.000</td> <td> 4.63e+06</td> <td> 1.63e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>revenue</th>         <td>    0.8408</td> <td>    0.002</td> <td>  363.356</td> <td> 0.000</td> <td>    0.836</td> <td>    0.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>runtime</th>         <td>-2.558e+05</td> <td> 1.84e+04</td> <td>  -13.921</td> <td> 0.000</td> <td>-2.92e+05</td> <td> -2.2e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ta</th>              <td> 1.498e+07</td> <td> 5.22e+06</td> <td>    2.871</td> <td> 0.004</td> <td> 4.75e+06</td> <td> 2.52e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vote_average</th>    <td> 6.751e+06</td> <td> 4.12e+05</td> <td>   16.384</td> <td> 0.000</td> <td> 5.94e+06</td> <td> 7.56e+06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1379.766</td> <th>  Durbin-Watson:     </th> <td>   1.842</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14735.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.919</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>10.909</td>  <th>  Cond. No.          </th> <td>2.89e+09</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.89e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 return   R-squared:                       0.967\n",
       "Model:                            OLS   Adj. R-squared:                  0.967\n",
       "Method:                 Least Squares   F-statistic:                     9834.\n",
       "Date:                Fri, 05 Feb 2021   Prob (F-statistic):               0.00\n",
       "Time:                        16:44:55   Log-Likelihood:                -99053.\n",
       "No. Observations:                5364   AIC:                         1.981e+05\n",
       "Df Residuals:                    5347   BIC:                         1.983e+05\n",
       "Df Model:                          16                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -1.561e+07   3.19e+06     -4.890      0.000   -2.19e+07   -9.35e+06\n",
       "Action          -1.186e+07    9.6e+05    -12.348      0.000   -1.37e+07   -9.97e+06\n",
       "Adventure       -1.799e+07   1.36e+06    -13.218      0.000   -2.07e+07   -1.53e+07\n",
       "Animation       -2.156e+07   2.24e+06     -9.640      0.000   -2.59e+07   -1.72e+07\n",
       "Documentary      6.133e+06   3.74e+06      1.640      0.101    -1.2e+06    1.35e+07\n",
       "Family          -1.115e+07   3.49e+06     -3.192      0.001    -1.8e+07    -4.3e+06\n",
       "Fantasy         -1.421e+07    2.2e+06     -6.450      0.000   -1.85e+07   -9.89e+06\n",
       "Horror           7.271e+06    1.5e+06      4.838      0.000    4.32e+06    1.02e+07\n",
       "Science Fiction -9.354e+06   2.54e+06     -3.680      0.000   -1.43e+07   -4.37e+06\n",
       "War             -1.191e+07   4.15e+06     -2.869      0.004      -2e+07   -3.77e+06\n",
       "en              -1.259e+07    1.4e+06     -8.997      0.000   -1.53e+07   -9.85e+06\n",
       "fr              -6.644e+06      3e+06     -2.213      0.027   -1.25e+07   -7.57e+05\n",
       "hi               1.045e+07   2.97e+06      3.523      0.000    4.63e+06    1.63e+07\n",
       "revenue             0.8408      0.002    363.356      0.000       0.836       0.845\n",
       "runtime         -2.558e+05   1.84e+04    -13.921      0.000   -2.92e+05    -2.2e+05\n",
       "ta               1.498e+07   5.22e+06      2.871      0.004    4.75e+06    2.52e+07\n",
       "vote_average     6.751e+06   4.12e+05     16.384      0.000    5.94e+06    7.56e+06\n",
       "==============================================================================\n",
       "Omnibus:                     1379.766   Durbin-Watson:                   1.842\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14735.533\n",
       "Skew:                          -0.919   Prob(JB):                         0.00\n",
       "Kurtosis:                      10.909   Cond. No.                     2.89e+09\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.89e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=[]\n",
    "\n",
    "a = dict(est.pvalues)\n",
    "\n",
    "for i, pvalue in a.items():\n",
    "    if pvalue > 0.05:\n",
    "        z.append(i)\n",
    "\n",
    "X = X.drop(columns=z)\n",
    "y = df['return']\n",
    "x = sm.add_constant(X)\n",
    "\n",
    "est = sm.OLS(y, x).fit()\n",
    "est.summary()\n",
    "\n",
    "#tried a model after removing all high pvalue coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Movies Manual Regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using the normal equation $(X^T X)^{-1}X^Ty$.\n",
    "\n",
    "Verify that the coefficients are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -1.560550e+07\n",
       "1    -1.185606e+07\n",
       "2    -1.799015e+07\n",
       "3    -2.156394e+07\n",
       "4     6.133159e+06\n",
       "5    -1.114811e+07\n",
       "6    -1.420969e+07\n",
       "7     7.270747e+06\n",
       "8    -9.354353e+06\n",
       "9    -1.191073e+07\n",
       "10   -1.258946e+07\n",
       "11   -6.643560e+06\n",
       "12    1.044707e+07\n",
       "13    8.407762e-01\n",
       "14   -2.557538e+05\n",
       "15    1.498028e+07\n",
       "16    6.750593e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linalg.inv(x.T @ x) @ x.T @y\n",
    "\n",
    "#Coefficients match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Movies gradient descent regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using **gradient descent**. \n",
    "\n",
    "Hint: use `scipy.optimize` and remember we're finding the $\\beta$ that minimizes the squared loss function of linear regression: $f(\\beta) = (\\beta X - y)^2$. This will look like part 3 of this lecture.\n",
    "\n",
    "Verify your coefficients are similar to the ones in 2.1 and 2.2. They won't necessarily be exactly the same, but should be roughly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.73112427e+07, -1.15207434e+07, -1.77953009e+07, -2.18456926e+07,\n",
       "        6.00496754e+06, -1.12555085e+07, -1.40249812e+07,  7.51478250e+06,\n",
       "       -9.12604347e+06, -1.15524242e+07, -1.22085709e+07, -6.35160876e+06,\n",
       "        1.15438536e+07,  8.40420332e-01, -2.72148045e+05,  1.57955460e+07,\n",
       "        7.24684792e+06])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhat = np.zeros(x.shape[1])\n",
    "\n",
    "def gradient_descent(x,y,bhat):\n",
    "    return np.sum((bhat @ x - y) ** 2)\n",
    "\n",
    "probit_est = minimize(gradient_descent, bhat, args=(y,x), method='Powell')\n",
    "\n",
    "probit_est['x']\n",
    "\n",
    "#Coeffiecients do not match but are roughly similar I suppose"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIPzivigAhB3FeR6Q96N8T",
   "collapsed_sections": [],
   "name": "Workshop: Maximum likelihood.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
