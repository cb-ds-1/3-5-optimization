{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steven Zajac-Descôteaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bisection\n",
    "\n",
    "\n",
    "One of the most common algorithms for numerical root-finding is *bisection*.\n",
    "\n",
    "To understand the idea, recall the well-known game where:\n",
    "\n",
    "- Player A thinks of a secret number between 1 and 100  \n",
    "- Player B asks if it’s less than 50  \n",
    "  \n",
    "  - If yes, B asks if it’s less than 25  \n",
    "  - If no, B asks if it’s less than 75  \n",
    "  \n",
    "\n",
    "And so on.\n",
    "\n",
    "This is bisection, a relative of [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm). It works for all sufficiently well behaved increasing continuous functions with $ f(a) < 0 < f(b) $. \n",
    "\n",
    "Write an implementation of the bisection algorith, `bisect(f, lower, upper, tol)` which, given a function `f`, a lower bound `lower` and an upper bound `upper` finds the point `x` where `f(x) = 0`. The parameter `tol` is a numerical tolerance, you should stop once your step size is smaller than `tol`.\n",
    "\n",
    "\n",
    "Use it to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1 \\tag{2}\n",
    "$$\n",
    "\n",
    "in python: `lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1`\n",
    "\n",
    "The value where f(x) = 0 should be around `0.408`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Solution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40829350427936706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.math.ubc.ca/~pwalls/math-python/roots-optimization/bisection/\n",
    "\n",
    "def bisect(f,lower,upper,tol):\n",
    "    \n",
    "    \"\"\"\n",
    "    f is given function\n",
    "    lower is lower bound\n",
    "    upper is upper bound\n",
    "    tol is numerical tolerance\n",
    "    \n",
    "    Approximate solution of f(x)=0 on interval [a,b] by bisection method\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if f(lower)*f(upper) >= 0:\n",
    "        print('Bisection method fails.')\n",
    "        return None\n",
    "    \n",
    "    a = lower\n",
    "    b = upper\n",
    "    \n",
    "    for n in range(1,tol+1): #number of iterations or tol chosen. (run this until tol is reached)\n",
    "        mid = (a + b)/2 #midpoint\n",
    "        f_mid = f(mid)\n",
    "        \n",
    "        #Determine the next subinterval\n",
    "        if f(a) * f_mid < 0: #[a1,b1]: a1=a0 and b1=mid0\n",
    "            a = a\n",
    "            b = mid\n",
    "        elif f(b) * f_mid < 0: #[a1,b1]: a1=mid0 and b1=b0\n",
    "            a = mid\n",
    "            b = b\n",
    "        elif f_mid == 0: #exact solution if f_mid=0\n",
    "            print('Exact Solution')\n",
    "            return mid\n",
    "        else:\n",
    "            print('Bisection method fails.')\n",
    "            return None\n",
    "    \n",
    "    return (a + b)/2 #return the midpoint value at tol or N mN = (aN + bN)/2\n",
    "\n",
    "\n",
    "f = lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "lower = 0\n",
    "upper = 1\n",
    "tol = 1000\n",
    "\n",
    "bisect(f,lower,upper,tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 (stretch) Recursive Bisect\n",
    "\n",
    "Write a recursive version of the bisection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect_rec(f, lower, upper, tol):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Movies Regression\n",
    "\n",
    "Write the best linear regression model you can on the [Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset?select=ratings.csv) to predict the profitability of a movie (revenue - budget). Maintain the interpretability of the model.\n",
    "\n",
    "Few notes:\n",
    "\n",
    "1. Clean your data! Movies where the budget or revenue are invalid should be thrown out\n",
    "\n",
    "2. Be creative with feature engineering. You can include processing to one-hot encode the type of movie, etc.\n",
    "\n",
    "3. The model should be useful for someone **who is thinking about making a movie**. So features like the popularity can't be used. You could, however, use the ratings to figure out if making \"good\" or \"oscar bait\" movies is a profitable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPi9xcx3mywk"
   },
   "outputs": [],
   "source": [
    "movies_url = {\n",
    "\"movies_metadata\": \"1RLvh6rhzYiDDjPaudDgyS9LmqjbKH-wh\",\n",
    "\"keywords\": \"1YLOIxb-EPC_7QpkmRqkq9E6j7iqmoEh3\",\n",
    "\"ratings\": \"1_5HNurSOMnU0JIcXBJ5mv1NaXCx9oCVG\",\n",
    "\"credits\": \"1bX9othXfLu5NZbVZtIPGV5Hbn8b5URPf\",\n",
    "\"ratings_small\": \"1fCWT69efrj4Oxdm8ZNoTeSahCOy6_u6w\",\n",
    "\"links_small\": \"1fh6pS7XuNgnZk2J3EmYk_9jO_Au_6C15\",\n",
    "\"links\": \"1hWUSMo_GwkfmhehKqs8Rs6mWIauklkbP\",\n",
    "}\n",
    "\n",
    "def read_gdrive(url):\n",
    "    \"\"\"\n",
    "    Reads file from Google Drive sharing link\n",
    "    \"\"\"\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3263: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "movies_metadata = read_gdrive('1RLvh6rhzYiDDjPaudDgyS9LmqjbKH-wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For later\n",
    "def recur_elim(model, X, pval=0.05): \n",
    "    \"\"\"\n",
    "    Use recursion to eliminate p-values that are above 0.05 at least.\n",
    "    Or as chosen.\n",
    "    \"\"\"\n",
    "    global keep #Need this to be able to use it later? (I think)\n",
    "    \n",
    "    new_pval= pd.DataFrame(model.pvalues) #Put pvalues into df\n",
    "    new_pval.columns = ['pvalue'] #rename col\n",
    "    \n",
    "    coefs = pd.DataFrame(model.params) #Put coef names\n",
    "    coefs.columns = ['coefs'] #name col\n",
    "\n",
    "    #Did recursion first and base case last \n",
    "    if len(new_pval[new_pval['pvalue'] > pval].index) > 0:\n",
    "        #Drop where pvalue is greater than input limit of p-val\n",
    "        drop_Pval = (new_pval[new_pval['pvalue'] > pval].index)\n",
    "        X_dropped = X.drop(columns=drop_Pval) #New X without high p-vals\n",
    "        model = sm.OLS(y, X_dropped).fit(cov_type='HC2')\n",
    "        \n",
    "        return recur_elim(model, X_dropped)\n",
    "    \n",
    "    #Base case: no mre p-vals > input p-val\n",
    "    #Length of new_pval list where pvals are larger than input\n",
    "    else:\n",
    "        keep = list(X.columns.values)\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies_metadata.copy()\n",
    "\n",
    "movies['belongs_to_collection'] = movies['belongs_to_collection'].notna().astype(int)\n",
    "\n",
    "#Drop columns that are irrelevant, have too much missing or are incorrect ie. status. \n",
    "movies.drop(['homepage','id','imdb_id','overview','tagline','poster_path','original_title',\n",
    "            'video','spoken_languages','production_companies','vote_count','vote_average','popularity',\n",
    "            'adult','status','runtime'],axis=1,inplace=True)\n",
    "\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "movies.budget = movies.budget.astype(float)\n",
    "\n",
    "#keep only where values are not 0\n",
    "movies = movies[movies['budget'] != 0]\n",
    "movies = movies[movies['revenue'] != 0 ]\n",
    "\n",
    "#Make a profit col \n",
    "movies['profit'] = movies['revenue'].subtract(movies['budget'])\n",
    "\n",
    "#col = ['spoken_languages','production_companies','production_countries','genres']\n",
    "\n",
    "#for c in col:\n",
    "#    movies[c] = movies[c].fillna(np.nan).apply()\n",
    "#    movies[c] = movies[c].apply(lambda x: [i['name']for i in x])\n",
    "\n",
    "movies['release_date'] = pd.to_datetime(movies['release_date'])\n",
    "\n",
    "#Convert release date to season category using dictionary and mapping\n",
    "seasons = ['win','win','spr','spr','spr','sum','sum','sum','fall','fall','fall','win']\n",
    "month_season = dict(zip(range(1,13),seasons))\n",
    "\n",
    "#movies.release_date = movies.release_date.dt.month.map(month_season) --> added below to remove a step\n",
    "movies[['spr_rel','sum_rel','win_rel']] = pd.get_dummies(movies.release_date.dt.month.map(month_season),\n",
    "                                                         drop_first=True)\n",
    "\n",
    "#Get first genre. Assuming like an ingredient list on packaging, first genre is most pertinent \n",
    "def get_genre(data):\n",
    "    for g in eval(data):\n",
    "        return g['name']\n",
    "\n",
    "def get_countries(data):\n",
    "    for c in eval(data):\n",
    "        return c['name']\n",
    "\n",
    "#Convert genre1 as category.......need to make sure maybe OHE\n",
    "movies['genre'] = (movies.genres.apply(get_genre)).astype('category')\n",
    "movies['country'] = (movies.production_countries.apply(get_countries)).astype('category')\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "#If english vs other language. \n",
    "movies['lang_en'] = (movies.original_language=='en').astype(int)\n",
    "\n",
    "movies.drop(['revenue','genres','release_date','title','original_language',\n",
    "             'production_countries'],axis=1,inplace=True)\n",
    "\n",
    "movies = pd.get_dummies(movies,drop_first=True) #Convert Genre col in dummies\n",
    "\n",
    "movies['const'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:1830: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 31, but rank is 30\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>profit</td>      <th>  R-squared:         </th>  <td>   0.406</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.403</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>4.235e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 03 Feb 2021</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:40:10</td>     <th>  Log-Likelihood:    </th> <td>-1.0582e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5313</td>      <th>  AIC:               </th>  <td>2.117e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5281</td>      <th>  BIC:               </th>  <td>2.119e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC2</td>       <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                  <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>belongs_to_collection</th>            <td>  8.22e+07</td> <td> 4.53e+06</td> <td>   18.153</td> <td> 0.000</td> <td> 7.33e+07</td> <td> 9.11e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>budget</th>                           <td>    1.8536</td> <td>    0.104</td> <td>   17.814</td> <td> 0.000</td> <td>    1.650</td> <td>    2.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Animation</th>                  <td>  4.74e+07</td> <td> 1.64e+07</td> <td>    2.888</td> <td> 0.004</td> <td> 1.52e+07</td> <td> 7.96e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Comedy</th>                     <td> 1.195e+07</td> <td> 2.98e+06</td> <td>    4.009</td> <td> 0.000</td> <td> 6.11e+06</td> <td> 1.78e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Documentary</th>                <td> 2.372e+07</td> <td> 6.52e+06</td> <td>    3.638</td> <td> 0.000</td> <td> 1.09e+07</td> <td> 3.65e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Drama</th>                      <td> 1.363e+07</td> <td> 3.25e+06</td> <td>    4.192</td> <td> 0.000</td> <td> 7.26e+06</td> <td>    2e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Foreign</th>                    <td> 1.158e+07</td> <td> 4.84e+06</td> <td>    2.393</td> <td> 0.017</td> <td>  2.1e+06</td> <td> 2.11e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_History</th>                    <td> 3.127e+07</td> <td> 1.09e+07</td> <td>    2.878</td> <td> 0.004</td> <td> 9.98e+06</td> <td> 5.26e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_Romance</th>                    <td> 2.843e+07</td> <td> 8.28e+06</td> <td>    3.432</td> <td> 0.001</td> <td> 1.22e+07</td> <td> 4.47e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>genre_TV Movie</th>                   <td> 7.094e+07</td> <td> 3.78e+06</td> <td>   18.780</td> <td> 0.000</td> <td> 6.35e+07</td> <td> 7.83e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Bahamas</th>                  <td> 6.857e+07</td> <td> 1.83e+07</td> <td>    3.740</td> <td> 0.000</td> <td> 3.26e+07</td> <td> 1.05e+08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Burkina Faso</th>             <td> 2.941e+07</td> <td> 3.59e+06</td> <td>    8.200</td> <td> 0.000</td> <td> 2.24e+07</td> <td> 3.64e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Chile</th>                    <td> 2.806e+07</td> <td> 6.54e+06</td> <td>    4.289</td> <td> 0.000</td> <td> 1.52e+07</td> <td> 4.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Czech Republic</th>           <td>-6.296e+07</td> <td> 2.94e+07</td> <td>   -2.141</td> <td> 0.032</td> <td>-1.21e+08</td> <td>-5.32e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_India</th>                    <td> 3.427e+07</td> <td> 4.81e+06</td> <td>    7.121</td> <td> 0.000</td> <td> 2.48e+07</td> <td> 4.37e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Indonesia</th>                <td>-4.804e+07</td> <td> 7.08e+06</td> <td>   -6.784</td> <td> 0.000</td> <td>-6.19e+07</td> <td>-3.42e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Iran</th>                     <td> 2.996e+07</td> <td> 3.63e+06</td> <td>    8.253</td> <td> 0.000</td> <td> 2.28e+07</td> <td> 3.71e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Malaysia</th>                 <td> 6.384e+07</td> <td> 3.07e+06</td> <td>   20.817</td> <td> 0.000</td> <td> 5.78e+07</td> <td> 6.99e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Mali</th>                     <td> 2.398e+07</td> <td> 3.54e+06</td> <td>    6.780</td> <td> 0.000</td> <td>  1.7e+07</td> <td> 3.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Malta</th>                    <td>-3.006e+07</td> <td> 3.68e+06</td> <td>   -8.167</td> <td> 0.000</td> <td>-3.73e+07</td> <td>-2.28e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_New Zealand</th>              <td> 1.373e+08</td> <td> 5.46e+07</td> <td>    2.517</td> <td> 0.012</td> <td> 3.04e+07</td> <td> 2.44e+08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Peru</th>                     <td> 8.516e+07</td> <td> 3.39e+06</td> <td>   25.150</td> <td> 0.000</td> <td> 7.85e+07</td> <td> 9.18e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Philippines</th>              <td> 3.838e+07</td> <td> 6.25e+06</td> <td>    6.144</td> <td> 0.000</td> <td> 2.61e+07</td> <td> 5.06e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Qatar</th>                    <td> 2.611e+07</td> <td> 3.57e+06</td> <td>    7.312</td> <td> 0.000</td> <td> 1.91e+07</td> <td> 3.31e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Serbia</th>                   <td> 3.827e+07</td> <td> 4.78e+06</td> <td>    8.010</td> <td> 0.000</td> <td> 2.89e+07</td> <td> 4.76e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_South Korea</th>              <td> 3.379e+07</td> <td> 9.39e+06</td> <td>    3.598</td> <td> 0.000</td> <td> 1.54e+07</td> <td> 5.22e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Sweden</th>                   <td> 2.342e+07</td> <td> 6.43e+06</td> <td>    3.641</td> <td> 0.000</td> <td> 1.08e+07</td> <td>  3.6e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Taiwan</th>                   <td>  3.28e+07</td> <td> 4.97e+06</td> <td>    6.606</td> <td> 0.000</td> <td> 2.31e+07</td> <td> 4.25e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_United Kingdom</th>           <td> 3.619e+07</td> <td> 6.53e+06</td> <td>    5.546</td> <td> 0.000</td> <td> 2.34e+07</td> <td>  4.9e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_United States of America</th> <td> 2.118e+07</td> <td> 3.09e+06</td> <td>    6.858</td> <td> 0.000</td> <td> 1.51e+07</td> <td> 2.72e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_Uruguay</th>                  <td>-1.782e+08</td> <td> 1.15e+07</td> <td>  -15.522</td> <td> 0.000</td> <td>-2.01e+08</td> <td>-1.56e+08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                            <td> -4.32e+07</td> <td> 4.13e+06</td> <td>  -10.465</td> <td> 0.000</td> <td>-5.13e+07</td> <td>-3.51e+07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4718.058</td> <th>  Durbin-Watson:     </th>  <td>   1.950</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>433426.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.832</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>46.579</td>  <th>  Cond. No.          </th>  <td>3.74e+09</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors are heteroscedasticity robust (HC2)<br/>[2] The condition number is large, 3.74e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 profit   R-squared:                       0.406\n",
       "Model:                            OLS   Adj. R-squared:                  0.403\n",
       "Method:                 Least Squares   F-statistic:                 4.235e+04\n",
       "Date:                Wed, 03 Feb 2021   Prob (F-statistic):               0.00\n",
       "Time:                        15:40:10   Log-Likelihood:            -1.0582e+05\n",
       "No. Observations:                5313   AIC:                         2.117e+05\n",
       "Df Residuals:                    5281   BIC:                         2.119e+05\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:                  HC2                                         \n",
       "====================================================================================================\n",
       "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------\n",
       "belongs_to_collection              8.22e+07   4.53e+06     18.153      0.000    7.33e+07    9.11e+07\n",
       "budget                               1.8536      0.104     17.814      0.000       1.650       2.058\n",
       "genre_Animation                    4.74e+07   1.64e+07      2.888      0.004    1.52e+07    7.96e+07\n",
       "genre_Comedy                      1.195e+07   2.98e+06      4.009      0.000    6.11e+06    1.78e+07\n",
       "genre_Documentary                 2.372e+07   6.52e+06      3.638      0.000    1.09e+07    3.65e+07\n",
       "genre_Drama                       1.363e+07   3.25e+06      4.192      0.000    7.26e+06       2e+07\n",
       "genre_Foreign                     1.158e+07   4.84e+06      2.393      0.017     2.1e+06    2.11e+07\n",
       "genre_History                     3.127e+07   1.09e+07      2.878      0.004    9.98e+06    5.26e+07\n",
       "genre_Romance                     2.843e+07   8.28e+06      3.432      0.001    1.22e+07    4.47e+07\n",
       "genre_TV Movie                    7.094e+07   3.78e+06     18.780      0.000    6.35e+07    7.83e+07\n",
       "country_Bahamas                   6.857e+07   1.83e+07      3.740      0.000    3.26e+07    1.05e+08\n",
       "country_Burkina Faso              2.941e+07   3.59e+06      8.200      0.000    2.24e+07    3.64e+07\n",
       "country_Chile                     2.806e+07   6.54e+06      4.289      0.000    1.52e+07    4.09e+07\n",
       "country_Czech Republic           -6.296e+07   2.94e+07     -2.141      0.032   -1.21e+08   -5.32e+06\n",
       "country_India                     3.427e+07   4.81e+06      7.121      0.000    2.48e+07    4.37e+07\n",
       "country_Indonesia                -4.804e+07   7.08e+06     -6.784      0.000   -6.19e+07   -3.42e+07\n",
       "country_Iran                      2.996e+07   3.63e+06      8.253      0.000    2.28e+07    3.71e+07\n",
       "country_Malaysia                  6.384e+07   3.07e+06     20.817      0.000    5.78e+07    6.99e+07\n",
       "country_Mali                      2.398e+07   3.54e+06      6.780      0.000     1.7e+07    3.09e+07\n",
       "country_Malta                    -3.006e+07   3.68e+06     -8.167      0.000   -3.73e+07   -2.28e+07\n",
       "country_New Zealand               1.373e+08   5.46e+07      2.517      0.012    3.04e+07    2.44e+08\n",
       "country_Peru                      8.516e+07   3.39e+06     25.150      0.000    7.85e+07    9.18e+07\n",
       "country_Philippines               3.838e+07   6.25e+06      6.144      0.000    2.61e+07    5.06e+07\n",
       "country_Qatar                     2.611e+07   3.57e+06      7.312      0.000    1.91e+07    3.31e+07\n",
       "country_Serbia                    3.827e+07   4.78e+06      8.010      0.000    2.89e+07    4.76e+07\n",
       "country_South Korea               3.379e+07   9.39e+06      3.598      0.000    1.54e+07    5.22e+07\n",
       "country_Sweden                    2.342e+07   6.43e+06      3.641      0.000    1.08e+07     3.6e+07\n",
       "country_Taiwan                     3.28e+07   4.97e+06      6.606      0.000    2.31e+07    4.25e+07\n",
       "country_United Kingdom            3.619e+07   6.53e+06      5.546      0.000    2.34e+07     4.9e+07\n",
       "country_United States of America  2.118e+07   3.09e+06      6.858      0.000    1.51e+07    2.72e+07\n",
       "country_Uruguay                  -1.782e+08   1.15e+07    -15.522      0.000   -2.01e+08   -1.56e+08\n",
       "const                             -4.32e+07   4.13e+06    -10.465      0.000   -5.13e+07   -3.51e+07\n",
       "==============================================================================\n",
       "Omnibus:                     4718.058   Durbin-Watson:                   1.950\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           433426.389\n",
       "Skew:                           3.832   Prob(JB):                         0.00\n",
       "Kurtosis:                      46.579   Cond. No.                     3.74e+09\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC2)\n",
       "[2] The condition number is large, 3.74e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = movies.drop('profit',axis=1)\n",
    "y = movies['profit']\n",
    "\n",
    "model = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "\n",
    "recur_elim(model,X) #use recursion to drop p-values that are above 0.05\n",
    "\n",
    "X = movies[keep]\n",
    "y = movies.profit \n",
    "\n",
    "est = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Movies Manual Regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using the normal equation $(X^T X)^{-1}X^Ty$.\n",
    "\n",
    "Verify that the coefficients are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.21960906e+07],\n",
       "       [ 1.85358989e+00],\n",
       "       [ 4.73971514e+07],\n",
       "       [ 1.19481705e+07],\n",
       "       [ 2.37202919e+07],\n",
       "       [ 1.36322483e+07],\n",
       "       [ 1.15835657e+07],\n",
       "       [ 3.12662299e+07],\n",
       "       [ 2.84297369e+07],\n",
       "       [ 7.09365012e+07],\n",
       "       [ 6.85715031e+07],\n",
       "       [ 2.94105115e+07],\n",
       "       [ 2.80619933e+07],\n",
       "       [-6.29590579e+07],\n",
       "       [ 3.42746994e+07],\n",
       "       [-4.80436078e+07],\n",
       "       [ 2.99585562e+07],\n",
       "       [ 6.38423197e+07],\n",
       "       [ 2.39760226e+07],\n",
       "       [-3.00636738e+07],\n",
       "       [ 1.37334446e+08],\n",
       "       [ 8.51554876e+07],\n",
       "       [ 3.83828452e+07],\n",
       "       [ 2.61125355e+07],\n",
       "       [ 3.82663554e+07],\n",
       "       [ 3.37924868e+07],\n",
       "       [ 2.34233922e+07],\n",
       "       [ 3.28016577e+07],\n",
       "       [ 3.61886423e+07],\n",
       "       [ 2.11816222e+07],\n",
       "       [-1.78235675e+08],\n",
       "       [-4.32044506e+07]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/performing-linear-regression-using-the-normal-equation-6372ed3c57\n",
    "\n",
    "β = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) # normal equation\n",
    "#Need to use linalg from Numpy\n",
    "β.reshape(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "belongs_to_collection               8.219609e+07\n",
       "budget                              1.853590e+00\n",
       "genre_Animation                     4.739715e+07\n",
       "genre_Comedy                        1.194817e+07\n",
       "genre_Documentary                   2.372029e+07\n",
       "genre_Drama                         1.363225e+07\n",
       "genre_Foreign                       1.158357e+07\n",
       "genre_History                       3.126623e+07\n",
       "genre_Romance                       2.842974e+07\n",
       "genre_TV Movie                      7.093650e+07\n",
       "country_Bahamas                     6.857150e+07\n",
       "country_Burkina Faso                2.941051e+07\n",
       "country_Chile                       2.806199e+07\n",
       "country_Czech Republic             -6.295906e+07\n",
       "country_India                       3.427470e+07\n",
       "country_Indonesia                  -4.804361e+07\n",
       "country_Iran                        2.995856e+07\n",
       "country_Malaysia                    6.384232e+07\n",
       "country_Mali                        2.397602e+07\n",
       "country_Malta                      -3.006367e+07\n",
       "country_New Zealand                 1.373344e+08\n",
       "country_Peru                        8.515549e+07\n",
       "country_Philippines                 3.838284e+07\n",
       "country_Qatar                       2.611254e+07\n",
       "country_Serbia                      3.826636e+07\n",
       "country_South Korea                 3.379249e+07\n",
       "country_Sweden                      2.342339e+07\n",
       "country_Taiwan                      3.280166e+07\n",
       "country_United Kingdom              3.618864e+07\n",
       "country_United States of America    2.118162e+07\n",
       "country_Uruguay                    -1.782357e+08\n",
       "const                              -4.320445e+07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Movies gradient descent regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using **gradient descent**. \n",
    "\n",
    "Hint: use `scipy.optimize` and remember we're finding the $\\beta$ that minimizes the squared loss function of linear regression: $f(\\beta) = (\\beta X - y)^2$. This will look like part 3 of this lecture.\n",
    "\n",
    "Verify your coefficients are similar to the ones in 2.1 and 2.2. They won't necessarily be exactly the same, but should be roughly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_dscnt(bhat,y,X):\n",
    "    return np.sum( ((X.dot(bhat)) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.20073548e+07,  1.85336686e+00,  4.71574627e+07,  1.16399204e+07,\n",
       "        2.34212450e+07,  1.33664499e+07,  1.13060534e+07,  3.10368737e+07,\n",
       "        2.81658291e+07,  7.09325291e+07,  6.86613761e+07,  2.96714283e+07,\n",
       "        2.82351189e+07, -6.28205729e+07,  3.44394187e+07, -4.78569661e+07,\n",
       "        3.02193399e+07,  6.38412139e+07,  2.42369328e+07, -3.00588254e+07,\n",
       "        1.37485838e+08,  8.51603921e+07,  3.85510865e+07,  2.63735075e+07,\n",
       "        3.83367040e+07,  3.38819912e+07,  2.35589589e+07,  3.31049688e+07,\n",
       "        3.63733993e+07,  2.16180605e+07, -1.78207015e+08, -4.32034448e+07])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "X = movies[keep] #Constant already added \n",
    "y = movies.profit\n",
    "\n",
    "#create beta hat vector to maximize on\n",
    "#will store the values of maximum likelihood beta parameters\n",
    "#Set to rand rather than 0\n",
    "bhat = np.random.rand(np.shape(X)[1])\n",
    "\n",
    "#unvectorized MLE estimation\n",
    "gradient_est = minimize(gr_dscnt, bhat, args=(y,X), method='Powell')\n",
    "\n",
    "#print vector of maximized betahats\n",
    "gradient_est['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3983247565365491"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIPzivigAhB3FeR6Q96N8T",
   "collapsed_sections": [],
   "name": "Workshop: Maximum likelihood.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
